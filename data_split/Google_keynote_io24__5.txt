Now, because I'm a Gemini advanced user, this works on any PDF and takes full advantage of the long context window. And there's just lots of times when that's useful. For example, let's say you're looking for a quick answer in an appliance user manual. And there you have it. It turns out, no, spin serves are not allowed. So Gemini not only gives me a clear answer to my question, it also shows me exactly where on the PDF to learn more. Awesome. Okay. So that's a few of the ways that we're enhancing Gemini to do more context aware and helpful in the moment. And what you've seen here are the first really many new ways that Gemini will unlock new experiences at the system level. And they're only available on Android. You'll see these and work coming to hundreds of billions of devices over the next couple of months. Now, building Google AI directly into the OS elevates the entire smartphone experience. And Android is the first mobile operating system to include a built-in, on-device foundation model. This lets us bring Gemini goodness from the data center right into your pocket. So the experience is faster while also protecting your privacy. Starting with Pixel later this year, we'll be expanding what's possible with our latest model Gemini Nano with multimodality. This means your phone can understand the world the way you understand it. So not just through text input, but also through sites, sounds, and spoken language. Let me give you an example. 2.2 billion people experience blindness or low vision. So several years ago, we developed TalkBack, an accessibility feature that helps people navigate their phone through touch and spoken feedback. Helping with images is especially important. In fact, my colleague, Carro, who uses TalkBack will typically come across 90 unlabeled images per day. Thankfully, TalkBack makes them accessible. And now we're taking that to the next level with the multimodal capabilities of Gemini Nano. So when someone sends Carro a photo, she'll get a richer and clearer description of what's happening. Or let's say Carro is shopping online for an outfit. Now she can get a crystal clear description of the style and cut to find the perfect look. Running Gemini.nano on device helps minimize the latency and the model even works when there's no network connection. These improvements to TalkBack are coming later this year. Let me show you another example of what on device AI can unlock. People lost more than $1 trillion to fraud last year. And a scandal continues to evolve across text, phone calls, and even videos. Android can help protect you from the bad guys, no matter how they try to reach you. So let's say I get rudely interrupted by a Nano-Nano caller right in the middle of my presentation. Hello. Hi, I'm Paulie from Save More Being Security Department. Am I speaking to Dave? Yeah, this is Dave, kind of in the middle of something. We've detected some suspicious activity on your account. It appears someone is trying to make unauthorized charges. Oh yeah, what kind of charges? I can't give you specifics over the phone, but to protect your account, I'm going to help you transfer your money to a secure account we've set up for you. And look at this. My phone gives me a warning that this call might be a scam. Gemini Nano alerts me. The second it detects suspicious activity, like a bank asking me to move my money to keep it safe. And everything happens right on my phone, so the audio processing stays completely private to me and on my device. We're currently testing this feature and we'll have more updates to share later this summer. And we're really just scratching the surface of the kinds of fast, private experiences that on device AI unlocks. Later this year, Gemini will be able to more deeply understand the content of the screen without any information leaving your phone thanks to the on device model. So remember that pickleball example earlier? Gemini and Android will be able to automatically understand the conversation and provide relevant suggestions like where to find pickleball clubs near me. And this is a powerful concept that will work across many apps on your phone. In fact, later today of the developer keynote, you'll hear about how we're empowering our developer community with our latest AI on models and tools like Gemini Nano and Gemini in Android Studio. Also, stay tuned tomorrow for our upcoming Android 15 updates, which we can't wait to share with you. As we said with the outset, we're reimagining Android with Gemini at the core from your favorite apps to the OS itself. We're bringing the power of AI to every aspect of the smartphone experience. And with that, let me head over to Josh to share more on our use for developers. Thank you. Thanks, Dave. It's amazing to see Gemini Nano do all of that directly on an Android phone. That was our plan all along to create a natively multimodal Gemini in a range of sizes. So you all as developers can choose the one that works best for you. Throughout the morning, you've heard a lot about our Gemini 1.5 series. And I want to talk about the two models you can access today. 1.5 Pro, which is getting a series of quality improvements that go out right about now. And the brand new 1.5 Flash. Both are available today globally in over 200 countries and territories. You can go over to AI Studio or Vertex AI if you're Google Cloud customer to give them a try. Now, both of these models are natively multimodal. That means you can interleave text, images, audio and video as inputs, and pack that massive 1 million token context window. And if you go to AI.google dev today, you can sign up to try the 2 million token context window for 1.5 Pro. And we're also adding a bunch of new developer features starting with video frame extraction. That's going to be in the Gemini API parallel function calling so you can return more than one function call at a time. And my favorite context caching. So you can send all of your files to the model once and not have to recent them over and over again. That should make the long context even more useful and more affordable. It ships next month. Now, we're using Google's infrastructure to serve these models. So developers like all of you can get great prices. 1.5 Pro is $7 per 1 million tokens. And I'm excited to share that for prompts up to 128K, it'll be 50% less for $3.50. And 1.5 Flash will start at $0.35 per 1 million tokens. Now, one thing you might be wondering is which model is best for your use case? Here's how we've been thinking about it on the team. We use 1.5 Pro for complex tasks where you really want the highest quality response. And it's okay if it takes a little bit longer to come back. We're using 1.5 Flash for quick tasks where the speed of the model is what matters the most. And as a developer, you can go try them both out today and see what works best for you. Now, I'm going to show you how it works here in AI Studio, the fastest way to build with Gemini. And we'll pull it up here. And you can see this is AI Studio. It's free to use. You don't have to configure anything to get going. You just go to AIStudio.Google.com, log in with your Google account, and you can just pick the model here in the right that works best for you. So one of the ways we've been using 1.5 Flash is to actually learn from customer feedback about some of our labs products. Flash makes this possible with its low latency. So what we did here is we just took a bunch of different feedback from our customer forums. You can put it into Flash, load up a prompt, and hit Run. Now, in the background, what it's going to do is it's going to go through that 93,000 token pile of information. And you can see here, start streaming it back. Now, this is really helpful because it pulls out the themes for us. It gives us all the right places where we can start to look. And you can see this is from some of the benefits from notebook LM, like we showed earlier. Now, what's great about this is that you can take something like this in AI Studio, prototype here in 10 seconds. And with one click in the upper left, get an API key, or over here in the upper right, just tap Get Code. And you've got all of the model configurations, the safety settings ready to go straight into your IDE. Now, over time, if you find that you need more enterprise grade features, you can use the same Gemini 1.5 models and the same configurations right in Vertex AI. That way, you can scale up with Google Cloud as your enterprise needs grow. So that's our newly updated Gemini 1.5 Pro and the new 1.5 Flash, both of which are available today globally. And you'll hear a lot more about them in the developer keynote later today. Now, let's shift gears and talk about Gemma, our family of open models, which are crucial for driving AI innovation and responsibility. Gemma is built from the same research and technology as Gemini. It offers top performance and comes in lightweight 7B and 2B sizes. Now, since it launched less than three months ago, it's been downloaded millions of times across all the major model hubs. Developers and researchers have been using it and customizing the base Gemma model and using some of our pre-trained variants like recurrent Gemma and code Gemma. And today's newest member, PolyGema, our first vision language open model and it's available right now. It's optimized for a range of image captioning visual Q&A and other image labeling tasks. So go give it a try. I'm also too excited to announce that we have Gemma 2 coming. It's the next generation of Gemma and it will be available in June. One of the top requests we've heard from developers is for a bigger Gemma model, but it's still going to fit in a size that's easy for all of you to use. So in a few weeks, we'll be adding a new 27 billion parameter model to Gemma 2. And here's what's great about it. This size is optimized by Nvidia to run on next gen GPUs and can run efficiently on a single TPU host in Vertex AI. So this quality to size ratio is amazing because it'll outperform models more than twice its size. We can't wait to see what you're going to build with it. To wrap up, I want to share this inspiring story from India where developers have been using Gemma and its unique tokenization to create Navarasa, a set of instruction tuned models to expand access to 15 Indic languages. This builds on our efforts to make information accessible in more than 7,000 languages around the world. Take a look. Languages are a very trusting problem to solve actually. And given India has a huge variety of languages and it changes every 5 kilometers. When technology is developed for a particular culture, it won't be able to solve and understand the nuances of a country like India. One of Gemma's features is an incredibly powerful tokenizer which enables the model to use hundreds of thousands of words, symbols and characters across so many alphabets and language systems. This large vocabulary is critical to adapting Gemma to power projects like Navarasa. Navarasa is a model that's trained for Indic languages. It's a fine tune model based on Google's Gemma. We built Navarasa to make large language models culturally rooted where people can talk in their native language and get the responses in their native language. Our biggest dream is to build a model to include everyone from all corners of India. We should use AI to make sure that no one is following and no one is using it. The end of the video. Listening to everything that's been announced today is clear that AI is already helping people from their everyday tasks to their most ambitious, productive and imaginative endeavors. Our AI innovations like multiple-dality, long-context and agents are the cutting edge of what this technology can do, taking it to a whole new level is capacity to help people. Yet, as with any emerging technology, there are still risks and new questions that will arise as air advances and its uses evolve. In navigating these complexities, we're guided by our AI principles and we're learning from our users, partners and our own research. To us, building AI responsibly means both addressing the risks and maximizing the benefits for people and society. Let me begin with what we're doing to address the risks. Here, I want to focus on how we're improving our models and protecting against their misuse. Beyond what Demis shared earlier, we're improving our models with an industry standard practice called red teaming, in which we test our own models and try to break them to identify weaknesses. Adding to this work, we're developing a cutting edge technique we call AI-assisted red teaming. This draws on Google DeepMind's gaming breakthroughs like AlphaGo, where we train AI agents to compete against each other and improve and expand the scope of their red teaming capabilities. We're developing AI models with these capabilities to help address adversarial prompting and limit problematic outputs. We're also improving our models with feedback from two important groups, thousands of internal safety experts with a range of disciplines and a range of independent experts from academia to civil society. Both groups help us identify emerging risks from cybersecurity threats to potentially dangerous capabilities in areas like KEM Bio. Combining human insight with our safety testing methods will help make our models and products more accurate, reliable, and safer. This is particularly important as technical advances like better intonation make interactions with AI feel and sound more human-like. We're doing a lot of research in this area, including the potential for harm and misuse. We're also developing new tools to help prevent the misuse of our models. For example, imagine 3NVO create more realistic imagery and videos. We must also consider how there might be misuse to spread misinformation. To help last year we introduced Synth ID, a tool that adds imperceptible watermarks to our AI generated images and audio so that they're easier to identify. Today we're expanding Synth ID to two new modalities, text and video. These launches build on our efforts to deploy state-of-the-art watermarking capabilities across modalities. Moving forward will keep integrating advances like watermarking and other emerging techniques to secure our latest generations of Jemeni, Imagine, Luria, and VR models. We're also committed to working in the ecosystem with all of you to help others build on the advances we're making, and in the coming months we'll be open sourcing Synth ID text watermarking. This will be available in our updated responsible, generative AI toolkit which will create it to make it easier for developers to build AI responsibly. We're also collaborating with C2PA and we're supporting C2PA collaborating with Adobe, Microsoft, startups, and many others to build that implement standards that improve the transparency of digital media. Now let's turn to the second and equally important part of our responsible AI approach. How are we building AI to benefit people and society? Today our AI advances are helping to solve real world problems like accelerating the work of 1.8 million scientists in 190 countries who are using alpha-folds to work on issues like neglected diseases. Helping predict floods in more than 80 countries and helping organizations like the United Nations track progress of the world's 17 sustainable development goals with data commons. And now generative AI is unlocking new ways for us to make the world's information and knowledge universally accessible and useful for learning. Billions of people already use Google products to learn every day and generative AI is opening up new possibilities allowing us to ask questions like, what if everyone everywhere could have their own personal AI tutor on any topic or what if every educator could have the role assistant in the classroom. Today marks a new chapter for learning and education at Google. I'm excited to introduce LearnLam. Our new family of models based on Gemini and fine tuned for learning. LearnLam is grounded in educational research making learning experiences more personal and engaging. And it's coming to the products you use every day like search, Android, Gemini and YouTube. In fact, you've already seen LearnLam on stage today when it helps to me with his son's homework on Android. Now let's see how this works in the Gemini app. Early assessing to just gems, custom versions of Gemini that can act as personal, assistive experts on any topic. We're developing some pre-made gems which will be available in the Gemini app and web experience including one called Learning Coach. With Learning Coach you can get step-by-step study guidance along with helpful practice and memory techniques designed to build understanding rather than just give you the answer. Let's say you're a college student studying for an upcoming biology exam. If you need a tip to remember the formula for photosynthesis, Learning Coach can help. Learning Coach along with other pre-made gems will launch a Gemini in the coming months. And you can imagine what features like Gemini live can unlock for learning. Another example is a new feature in YouTube that uses LearnLam to make educational videos more interactive, allowing you to ask a clarifying question, get a helpful explanation or take a quiz. This even works for those long lectures or seminars thanks to Gemini models long context capabilities. This feature in YouTube is already rolling out to select Android users. As we work to extend LearnLam beyond our own products, we're partnering with experts and institutions like Columbia Teachers College, Arizona State University and Khan Academy to test and improve the new capabilities in our models for learning. And we've collaborated with MIT Rays to develop an online course to help educators better understand and use generative AI. We're also working directly with educators to build more helpful generative AI tools with LearnLam. For example, in Google Classroom, we're drawing on the advances we've heard about today to develop new ways to simplify and improve lesson planning. And enable teachers to tailor lessons and content to meet the individual needs of their students. Standing here today makes me think back to my own type as an undergraduate. Then AI was considered speculative far from any real world uses. Today we can see how much is already real. How much is already helping people from the everyday tasks to their most ambitious, productive and imaginative endeavors. And how much more is still to come? This is what motivates us. I'm excited about what's ahead and what will build with all of you. Back to you, Sundar. Thanks, James. All of this shows the important progress we've made as we take a bold and responsible approach to making AI helpful for everyone. Before we wrap, I have a feeling that someone out there might be counting how many times we have mentioned AI today. And since a big team today has been letting Google do the work for you, we went ahead and counted so that you don't have to. That might be a record in how many times someone has said AI. I'm tempted to say it a few more times, but I won't. Anyhow, this tally is more than just a punchline. It reflects something much deeper. We've been AI first in our approach for a long time. Our decades of research leadership have pioneered many of the modern breakthroughs that power AI progress for us and for the industry. On top of that, we have world-leading infrastructure built for the AI era. Cutting edge innovation in search now powered by Gemini, products that help with extraordinary scale, including 15 products with over half a billion users, and platforms that enable everyone, partners, customers, creators, and all of you to invent the future. This progress is only possible because of our incredible developer community. You're making it real through the experiences you build every day. So to everyone here in Showline and the millions more watching around the world, here's to the possibilities ahead and creating them together. Thank you. Hello, me to re-educe myself. My name is... What does this remind you of? Soap in your head. Wow! When all of these tools come together, it's a powerful combination. It's amazing. It's amazing. It's an entire suite of different kinds of possibilities. Hi, I'm Gemini. What neighborhood do you think I'm in? This appears to be the campus cross-area of London. Together we're creating a new era. Ah...