 Google's ambitions and artificial intelligence. Who will launch a Gemini? AI is rolling out to work. And it's completely changing the way we work. You know, a lot has happened in a year. There have been new beginnings. We found new ways to find new ideas and new solutions to age-old problems. Sorry about your shirt. We dreamt of things. Never too old for a tree house. We trained for things. I think it's called bubble. And learned about this thing. We found new paths. Took the next step. And made the big leap. Cannonball! We filled days like they were weeks. And more happened in months than it's happened in years. Oops! Reacts. Things got bigger. Like way bigger. And it wasn't all just for him, or for her. It was for everyone. And you know what? We're just getting started. Hi everyone. Good morning. Welcome to Google Iodes. It's great to have all of you with us. We have a few thousand developers with us here today at Shoreline. Millions more are joining virtually around the world. Thanks to everyone for being here. For those of you who haven't seen Iodes before, it's basically Google's version of the era store. But with fewer costume changes. At Google though, we are fully in our Gemini era. You'll hear a lot about that today. Before we get into it, I want to reflect on this moment we are in. We've been investing in AI for more than a decade. And innovating at every layer of the stack. Research, product, infrastructure. We're going to talk about it all today. Still, we are in the very early days of the AI platform shift. We see so much opportunity ahead. For creators, for developers, for startups, for everyone. Helping to drive those opportunities is what our Gemini era is all about. So let's get started. A year ago on this stage, we first shared our plans for Gemini. A frontier model built to be natively multi-model from the very beginning. That could reason across text, images, video, code and more. It's a big step in turning any input into any output. An IO for a new generation. Since then, we introduced the first Gemini models are most capable yet. They demonstrated state of the art performance on every multi-model benchmark. And that was just the beginning. Two months later, we introduced Gemini 1.5 Pro. Delivering a big breakthrough in long context. It can run 1 million tokens in production consistently, more than any other large scale foundation model yet. We want everyone to benefit from what Gemini can do. So we work quickly to share these advances with all of you. Today, more than 1.5 million developers use Gemini models across our tools. You're using it to debug code, get new insights, and build the next generation of AI applications. We've also been bringing Gemini's breakthrough capabilities across our products in powerful ways. We'll show examples today across search, photos, workspace, Android and more. Today, all of our two billion user products use Gemini. And we've introduced new experiences too. Including on mobile, where people can interact with Gemini directly through the app. Now available on Android and iOS, and through Gemini Advanced, which provides access to our most capable models. Over 1 million people have signed up to try it in just three months, and it continues to show strong momentum. One of the most exciting transformations with Gemini has been in Google Search. In the past year, we've answered billions of queries as part of her search-generative experience. People are using it to search in entirely new ways, and asking new types of questions, longer and more complex queries, even searching with photos, and getting back the best the web has to offer. We've been testing this experience outside of labs, and we're encouraged to see not only an increase in search usage, but also an increase in user satisfaction. I'm excited to announce that we will begin launching this fully revamped experience, AI overviews to everyone in the US this week, and we'll bring it to more countries soon. The so much innovation happening in search, thanks to Gemini, we can create much more powerful search experiences, including within our products. Let me show you an example in Google Photos. We launched Google Photos almost nine years ago. Since then, people have used it to organize their most important memories. Today, that amounts to more than 6 billion photos and videos uploaded every single day. And people are using photos to search across their life. With Gemini, you're making that a whole lot easier. Say you're at a parking station ready to pay, but you can't recall your license plate number. Before you could search photos for keywords, then scroll through years worth of photos looking for the right one. Now, you can simply ask photos. It knows the cards that appear often, it triangulates which one is yours, and just tells you the license plate number. And ask photos can also help you search your memories in a deeper way. For example, you might be reminiscing about your daughter Lucius early milestones. You can ask photos when did Lucius learn to swim. You can even fall up with something more complex. Show me how Lucius swimming has progressed. Here, Gemini goes beyond a simple search, recognizing different contexts from doing laps in the pool, to snorkeling in the ocean, to the texture and dates on her swimming certificates, and photos packages it up all together in a summary. You can really take it all in and relive amazing memories all over again. We are rolling out asked photos this summer with more capabilities to come. APPLAUSE Unlocking knowledge across formats is why we build Gemini to be multi-model from the ground up. It's one model with all the modalities built in. So not only does it understand each type of input, it finds connections between them. Multi-modality radically expands the questions we can ask, and the answers we will get back. Long context takes this a step further, enabling us to bring in even more information, hundreds of pages of text, hars of audio, a full hour of video, or entire code repos. Or if you want, roughly 96 cheesecake factory menus. For that many menus, you need a one-million token context window, now possible with Gemini 1.5 Pro. Developers have been using it in super interesting ways. Let's take a look. I remember the announcement, the one-million token context window, and my first reaction was, there's no way they were able to achieve this. I wanted to test this technical skills. So I uploaded a line chart. It was temperatures between Tokyo and Berlin, and how they were across the 12 months of the year. So I got in there, and I threw in the Python library that I was really struggling with. And I just asked it a simple question. And it nailed it. It could find specific references, the comments in the code, and specific requests that people had made, and other issues that people had had, but then suggest a fix for it, that related to what I was working on. I immediately tried to kind of crash it. So I took, you know, four or five research papers out on my desktop. And it's a mind-blowing experience when you add so much text, and then you see the kind of amount of tokens. You add is not even at half the capacity. It felt a little bit like Christmas, because you saw things kind of peppered up to the top of your feed about like, oh wow, I built this thing. Or, oh, it's doing this, and I would have never expected. Can I shoot a video of my possessions and turn that into a searchable database? So I ran to my bookshelf, and I shot a video just panning my camera along the bookshelf, and I fed the video into the model. It gave me the titles and authors of the books, even though the authors weren't visible on those bookspines, and on the bookshelf there was a squirrel nutcracker sat in front of the book, truncating the title. It would cite C, and it still guessed the correct book. The range of things you can do with that is almost unlimited. And so at that point for me was just like a click, like, this is it. I thought like I had like a super part in my hands. It was poetry, it was beautiful, I was so happy. It just, this, this is going to be amazing. This is, this is going to help people. This is kind of where the future of language models are going. Personalized to you, not because you trained it to be personal to you, but personal to you because you can give it such a fast understanding of who you are. We've been growing our Gemini 1.5 Pro with long context and preview over the last few months. We made a series of quality improvements across translation coding and reasoning. You'll see these updates reflected in the model starting today. I'm excited to announce that we are bringing this improved version of Gemini 1.5 Pro to all developers globally. In addition today, Gemini 1.5 Pro with 1 million context is now directly available for consumers in Gemini Advanced and can be used across 35 languages. 1 million tokens is opening up entirely new possibilities. It's exciting, but I think we can push ourselves even further. So today, we are expanding the context window to 2 million tokens. We are making it available for developers in private preview. It's amazing to look back and see just how much progress we have made in a few months. This represents the next step on our journey towards the ultimate goal of infinite context. So far, we've talked about two technical advances, multi-modality and long context. Each is powerful on its own, but together, they unlock deeper capabilities and more intelligence. Let's see how this comes to life with Google Workspace. People are always searching their emails in Gmail. We are working to make it much more powerful with Gemini. Let's look at how. As a parent, you want to know everything that's going on with your child's school. Okay, maybe not everything. But you want to stay informed. Gemini can help you keep up. Now we can ask Gemini to summarize all recent emails from the school. In the background, it's identifying relevant emails, even analyzing attachments like PDFs. And you get a summary of the key points and action items. So helpful. Maybe you were traveling this week and you couldn't make the PTA meeting. The recording of the meeting is an hour long. If it's from Google Meet, you can ask Gemini to give you the highlights. There's a parent's group looking for volunteers. You're free that day. Of course, Gemini can draft a reply. There are countless other examples of how this can make life easier. Gemini 1.5 Pro is available today in Workspace Labs. And up and out we'll share more later on. We just looked at an example with text outputs. But with the multi-modal model, we can do so much more to show you an early demo of an audio output in notebook LM. Here's Josh. Hey everyone. Last year at I.O. we introduced notebook LM, a research and writing tool grounded in the information you give it. Since then, we've seen a lot of momentum with students and teachers using it. And today, Gemini 1.5 Pro is coming to notebook LM and it's great. Let me show you. So here we are in notebook LM. You can load it up with all the materials here on the left. In this notebook, I've been using it with my younger son and I've added some of his science worksheets, a few slide decks from his teacher, and even an open source textbook full of charts and diagrams. With 1.5 Pro, it instantly creates this notebook guide with a helpful summary and can generate a study guide, an FAQ, or even quizzes. But for my son, Jimmy, she really learns best when he can listen to something. So we've prototyped a new feature with Gemini and it's called audio overviews. Notebook LM is going to take all the materials on the left as input and output them into a lively science discussion personalized for him. Let's take a listen. So let's dive into physics. What's on deck for today? Well, we're starting with the basics. Force and motion. And that of course means we have to talk about Sir Isaac Newton and his three laws of motion. Ah, yes, the foundation for understanding how objects move and interact. Ah, yes, this is where multimodal really shines. Now it generated this audio discussion based on that text material. And what's amazing is that my son and I can join into the conversation and steer it whichever direction we want. When I tap join. Hold on, we have a question. What's up, Josh? Yeah, can you give my son Jimmy a basketball example? Hey, Jimmy, that's a fantastic idea. Basketball is actually a great way to visualize force and motion. Let's break it down. Okay, so first, imagine a basketball just sitting there on the court. It's not moving, right? That's because all the forces acting on it are balanced. The downward pull of grab. Pretty cool, right? I got to say, the first time my son heard this, you should have seen how big his eyes got. Because he was gripped. They were talking to him. He was learning science through the example of basketball, his favorite sport. Now what's interesting is under the hood, you saw that Jim and I had used some of the concepts of gravity or Isaac Newton. But nothing in there was about basketball. It connected the dots and created that age-appropriate example for him. And this is what's becoming possible with the power of Jim and I. You can give it lots of information in any format. And it can be transformed in a way that's personalized and interactive for you. Back to you, Sundar. Thanks, Shar. The demo shows the real opportunity with multimodality. Soon you'll be able to mix and match inputs and outputs. This is what we mean when we say it's an IO for a new generation. And I can see you all out there thinking about the possibilities. But what if we could go even further? That's one of the opportunities we see with AI agents. Let me take a step back and explain what I mean by that. I think about them as intelligent systems that show reasoning, planning and memory are able to think multiple steps ahead. Work across software and systems. All to get something done on your behalf. And most importantly, under your supervision. We are still in the early days and you'll seek glimpses of our approach throughout the day. But let me show you the kinds of use cases we are working hard to solve. Let's start with shopping. It's pretty fun to shop for shoes. And a lot less fun to return them when they don't fit. Imagine if Gemini could do all the steps for you. Searching your inbox for the receipt. Locating the order number from your email. Filling out a return form. And even scheduling a pickup. That's much easier, right? APPLAUSE Let's take another example that's a bit more complex.