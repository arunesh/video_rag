Say you just moved to Chicago. You can imagine Gemini and Chrome working together. To help you do a number of things to get ready. Organizing, reasoning, synthesizing on your behalf. For example, you will want to explore the city and find services nearby. From dry cleaners to dog lockers. You'll have to update your new address across dozens of websites. Gemini can work across these tasks and will prompt you for more information when needed. So you're always in control. That part is really important as we prototype these experiences. We are thinking hard about how to do it in a way that's private, secure, and works for everyone. These are simple use cases, but they give you a good sense of the types of problems you want to solve. By building intelligence systems that think ahead, reason, and plan all on your behalf. The power of Gemini with multi-modality, long context, and agents. Brings us closer to our ultimate goal. Making AI helpful for everyone. We see this as how we will make the most progress against our mission. Organizing the world's information across every input. Making it accessible via any output. And combining the world's information with the information in your world in a way that's truly useful for you. To fully realize the benefits of AI will continue to break new ground. Google DeepMind is hard at work. To share more, please welcome for the first time on the Iostage, Sir Demis. Thanks, Inda. It's so great to be here. Ever since I was a kid, playing chess for the England Junior Team, I've been thinking about the nature of intelligence. I was captivated by the idea of a computer that could think like a person. It's ultimately why I became a programmer and studied neuroscience. I co-founded DeepMind in 2010 with the goal of one day building AGI, artificial general intelligence, a system that has human level cognitive capabilities. I've always believed that if we could build this technology responsibly, its impact would be truly profound and it could benefit humanity in incredible ways. Last year, we reached a milestone on that path when we formed Google DeepMind, combining AI talent from across the company into one super unit. Since then, we built AI systems that can do an amazing range of things. From turning language, vision, into action for robots, navigating complex virtual reading environments, solving a limpiav level math problems, and even discovering thousands of new materials. Just last week, we announced our next generation AlphaFull model. It can predict the structure and interactions of nearly all of life's molecules, including how proteins interact with strands of DNA and RNA. This will accelerate vitally important biological and medical research from disease understanding to drug discovery. And all of this was made possible with the best infrastructure for the AI era, including our highly optimized tensor processing units. At the centre of our efforts is our Gemini model. It's built up from the ground up to be natively multimodal because that's how we interact with and understand the world around us. We built a variety of models for different use cases. You've seen how powerful Gemini 1.5 Pro is. But we also know from user feedback that some applications need lower latency and a lower cost to serve. So today, we're introducing Gemini 1.5 Flash. Flash. Flash is a lightweight model compared to Pro. It's designed to be fast and cost efficient to serve at scale, while still featuring multimodal reasoning capabilities and breakthrough long context. Flash is optimized for tasks where low latency and efficiency matter most. Along today, you can use 1.5 Flash and 1.5 Pro with up to 1 million tokens in Google AI Studio and Vertex AI. And developers can sign up to try 2 million tokens. We're so excited to see what all of you will create with it. And you'll hear a little more about Flash later on from Josh. We're very excited by the progress we've made so far with our family of Gemini models. We're always striving to push the state of the art even further. At any one time, we have many different models in training and we use our very large and powerful ones to help teach and train our production ready models. Together with user feedback, this cutting edge research will help us to build amazing new products for billions of people. For example, in December, we shared a glimpse into the future of how people would interact with multimodal AI. How this would end up powering a new set of transformative experiences. Today, we have some exciting new progress to share about the future of AI assistance that we're calling Project Astra. For a long time, we've wanted to build a universal AI agent that can be truly helpful in everyday life. Our work making this vision and reality goes back many years. It's why we made Gemini multimodal from the very beginning. An agent like this has to understand and respond to our complex and dynamic world just like we do. It would need to take in and remember what it sees so it can understand context and take action. And it would have to be proactive, teachable and personal so you can talk to it naturally without lag or delay. While we've made some great strides in developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Building on our Gemini model, we've developed agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events and caching this for efficient recall. We've also enhanced how they sound with a wider range of intonations. These agents better understand the context you're in and can respond quickly in conversation, making the pace and quality of interaction feel much more natural. Here's a video of our prototype which we'll see has two parts. Each part was captured in a single take in real time. Okay, let's do some tests. Tell me when you see something that makes sound. I see a speaker which makes sound. What is that part of the speaker called? That is the tweeter. It produces high frequency sounds. Give me a creative alliteration about these. Creative crayons color cheerfully. They certainly craft colorful creations. What does that part of the code do? This code defines encryption and decryption functions. It seems to use AESCBC encryption to encode and decode data based on a key and an initialization vector, IV. That's right. What neighborhood do you think I'm in? This appears to be the King's Cross area of London. It is known for its railway station and transportation connections. Do you remember where you saw my glasses? Yes, I do. Your glasses were on the desk near a red apple. What can I add here to make this system faster? Adding a cache between the server and database could improve speed. What does this remind you of? I think you'll agree. It's amazing to see how far AI has come. Especially for the people who have been working with AI for years. I think you'll agree. It's amazing to see how far AI has come. Especially for the people who have been working with AI for years. I think you'll agree to see how far AI has come. Especially when it comes to spatial understanding, video processing and memory. It's easy to envisage a future where you can have an expert assistant by your side through your phone or new exciting form factors like glasses. Some of these agent capabilities will come to Google products like Gemini app later this year. For those of you on site today, you can try out a live demo version of this experience in the AI sandbox area. Next, let's take a look at how our innovations are helping people bring new creative ideas to life. Today we're introducing a series of updates across our genitive media tools with new models covering image, music and video. In the past year, we've been enhancing quality, improving safety and increasing access to help tell this story. Here's done. Thanks, Demis. Over the past few months, we've been working hard to build a new image generation model from the ground up. With stronger evaluations, extensive red teaming and state of the art watermarking with synth ID. Today, I'm so excited to introduce Imagine Three. It's our most capable image generation model yet. Imagine Three is more photorealistic. You can literally count the whiskers on its snout. With richer details like this incredible sunlight in the shot and fewer visual artifacts or distorted images. It understands prompts written the way people write. The more creative and detailed you are, the better. And Imagine Three remembers to incorporate small details like the wildflowers or small blue bird in this longer prompt. Plus, this is our best model yet for rendering text, which has been a challenge for image generation models. The side-by-side comparisons independent evaluators preferred Imagine Three over other popular image generation models. In some, Imagine Three is our highest quality image generation model so far. You can sign up today to try Imagine Three in Image FX, part of our suite of AI tools at labs.google. And it'll be coming soon to developers and enterprise customers in Vertex AI. The area full of creative possibility is generative music. I've been working in this space for over 20 years, and this is by far the most exciting year of my career. We're exploring ways of working with artists to expand their creativity with AI. Together with YouTube, we've been building music AI sandbox, a suite of professional music AI tools that can create new instrumental sections from scratch, transfer styles between tracks, and more. To help us design and test them, we've been working closely with incredible musicians, songwriters, and producers. Some of them made even entirely new songs in ways that would have not been possible without these tools. Let's hear from some of the artists we've been working with. I'm going to put this right back into the music AI tool, the same. What happens if Haiti meets Brazil? Dude, I have no clue what's about to be sped out. This is what excites me. As the hip-hop producer, we dug in the crates. We play these vinyls, and the part with it is no vocal, we pull it, we sample it, and we create an entire song around that. So right now we dig in an infinite crate. It's endless. Where I found the AI really useful for me, this way to fill in the sparser elements of my loops. Okay, let's try Bongo's. We're going to put Viola. We're going to put Rhythmic Clapping. And we're going to see what happens there. And it makes it sound ironically at the end of the day a little more human. So then this is entirely Google's loops right here. These are glutes. It's like having like this weird friend that's just like, try this, try that. And then you're like, oh, okay, yeah, no, that's pretty dope. It's a group in, group in, group in, group in, up, up, up. The tools are capable, speeding up the process of what's in my head getting it out. You're able to move light speed with your creativity. This is amazing. Right there. I think this really shows what's possible when we work with the artist's community on the future of music. You can find some brand new songs from these acclaimed artists and songwriters on their YouTube channels now. There's one more area I'm really excited to share with you. Our teams have made some incredible progress in genitive video. Today, I'm excited to announce our newest, most capable genitive video model called VO. VO creates high quality 1080p videos from text, image and video prompts. It can capture the details of your instructions in different visual and cinematic styles. You can prompt for things like aerial shots of a landscape or time lapse and further edit your videos using additional prompts. You can use VO in our new experimental tool called Video FX. We're exploring features like storyboarding and generating longer scenes. VO gives you unprecedented creative control. Techniques for generating static images have come a long way, but generating video is a different challenge altogether. Not only is it important to understand where an object or subject should be in space, it needs to maintain this consistency over time, just like the car in this video. VO builds upon years of our pioneering genitive video model work, including GQN, FNACI, WALT, Video Poet, Lumiere and much more. We combine the best of these architectures and techniques to improve consistency, quality and output resolution. To see what VO can do, we put it in the hands of an amazing filmmaker. Let's take a look. Well, I've been interested in AI for a couple of years now. We got in contact with some of the people at Google and they had been working on something of their own. So we're all meeting here at Google Farms to make a short film. The core technology is Google DeepMind's generative video model that has been trained to convert input text into a video model. We are able to bring ideas to life that were otherwise not possible. We can visualize things on a time scale that's 10 or 100 times faster than before. When you're shooting, you can't really eat it, right? As much as you wish. And so we've been hearing that feedback that it allows for more functionality, more inspiration, more improvisation. But that's what's cool about it. It's like you can make a mistake faster. That's all you really want at the end of the day. At least in art, it's just to make mistakes fast. So using Gemini's multi-modal capabilities to optimize the model training process, VIO is able to better capture the nuance from prompts. So this includes cinematic techniques and visual effects, giving you total creative control. Everybody's going to become a director and everybody should be a director. Because at the heart of all of this is just storytelling. The closer we are to being able to tell each other, the closer we are to being able to tell each other, our stories the more we'll understand each other. These models are really enabling us to be more creative and to share that creativity with each other. So, let's go. Let's go. Let's go. Let's go. Let's go. Over the coming weeks, some of these features will be available to select creators through video effects at labs.google. And the wait list is open now. Of course, these advances in genitive video go beyond the beautiful visuals you've seen today. By teaching future AI models how to solve problems creatively, or in effect simulate the physics of our world, we can build more useful systems that can help people communicate in new ways, and thereby advance the frontiers of AI. When we first began this journey to build AI more than 15 years ago, we knew that one day it would change everything. Now that time is here. And we continue to be amazed by the progress we see and inspired by the advances still to come on the path to AI. Thanks and back to you, Sundar. Thanks, Dennis. A huge amount of innovation is happening at Google DeepMine. It's amazing how much progress we've made in the year. Training state of the art models requires a lot of computing power. Industry demand for ML compute has grown by a factor of 1 million in the last six years. And every year it increases 10 fold. Google was built for this. For 25 years, we have invested in world-class technical infrastructure. From the cutting edge hardware that powers search to our custom tenser processing units that power our AI advances. Gemini was trained and served entirely on our fourth and fifth generation TPUs. Another leading AI companies like Amtropic have trained their models on TPUs as well. Today, we are excited to announce the sixth generation of TPUs called Trillium. Trillium delivers a 4.7x improvement in compute performance per chip over the previous generation. So our most efficient and performant TPU today will make Trillium available to our cloud customers in late 2024. Alongside our TPUs, we are proud to offer CPUs and GPUs to support any workload that includes the new Axion processes we announced last month our first custom ARM-based CPU with industry leading performance and energy efficiency. We are also proud to be one of the first cloud providers to offer NVIDIAs cutting edge Blackwell GPUs available in early 2025. We are fortunate to have a long-standing partnership with NVIDIA, excited to bring Blackwell's capabilities to our customers. Chips are a foundational part of our integrated end-to-end system. From performance optimized hardware and open software to flexible consumption models, this all comes together in our AI hypercomputer, a groundbreaking supercomputer architecture. Businesses and developers are using it to tackle more complex challenges with more than twice the efficiency relative to just buying the raw hardware and chips. Our AI hypercomputer advancements are made possible in part because of our approach to liquid cooling in our data centers. We've been doing this for nearly a decade long before it became state of the art for the industry and today our total deployed fleet capacity for liquid cooling systems is nearly one gigawatt and growing. That's close to 70 times the capacity of any other fleet. And relying this is the sheer scale of our network which connects our infrastructure globally. Our network spans more than 2 million miles of terrestrial and subsea fiber over 10 times the reach of the next leading cloud provider. We'll keep making the investments necessary to advance AI innovation and deliver state of the art capabilities. And one of our greatest areas of investment and innovation is in our founding product search. 25 years ago we created search to help people make sense of the waves of information moving online. With each platform shift we have delivered breakthroughs to help answer your questions better. On mobile we unlock new types of questions and answers using better context, location awareness and real time information. With advances in natural language understanding and computer vision we enable new ways to search with your voice or a hum to find your new favorite song or an image of that flower you saw on your walk. Now you can even circle to search those cool new shoes you might want to buy. Go for it you can always return them later. Of course search in the Gemini era will take this to a whole new level.